+++ DeDoDe/descriptors/dedode_descriptor.py
@@ -42,8 +42,8 @@
         return {"descriptions": described_keypoints}
     
     def read_image(self, im_path, H = 784, W = 784):
-        return self.normalizer(torch.from_numpy(np.array(Image.open(im_path).resize((W,H)))/255.).permute(2,0,1)).cuda().float()[None]
+        return self.normalizer(torch.from_numpy(np.array(im_path.resize((W,H)))/255.).permute(2,0,1)).cuda().float()[None]
 
     def describe_keypoints_from_path(self, im_path, keypoints, H = 784, W = 784):
         batch = {"image": self.read_image(im_path, H = H, W = W)}
-        return self.describe_keypoints(batch, keypoints)
\ No newline at end of file
+        return self.describe_keypoints(batch, keypoints)

+++ DeDoDe/utils.py
@@ -67,7 +67,7 @@
     return x1_n
 
 @torch.no_grad()
-def finite_diff_hessian(f: tuple(["B", "H", "W"]), device = "cuda"):
+def finite_diff_hessian(f, device = "cuda"):
     dxx = torch.tensor([[0,0,0],[1,-2,1],[0,0,0]], device = device)[None,None]/2
     dxy = torch.tensor([[1,0,-1],[0,0,0],[-1,0,1]], device = device)[None,None]/4
     dyy = dxx.mT
@@ -77,7 +77,7 @@
     H = torch.stack((Hxx, Hxy, Hxy, Hyy), dim = -1).reshape(*f.shape,2,2)
     return H
 
-def finite_diff_grad(f: tuple(["B", "H", "W"]), device = "cuda"):
+def finite_diff_grad(f, device = "cuda"):
     dx = torch.tensor([[0,0,0],[-1,0,1],[0,0,0]],device = device)[None,None]/2
     dy = dx.mT
     gx = F.conv2d(f[:,None], dx, padding = 1)
@@ -85,11 +85,11 @@
     g = torch.cat((gx, gy), dim = 1)
     return g
 
-def fast_inv_2x2(matrix: tuple[...,2,2], eps = 1e-10):
+def fast_inv_2x2(matrix, eps = 1e-10):
     return 1/(torch.linalg.det(matrix)[...,None,None]+eps) * torch.stack((matrix[...,1,1],-matrix[...,0,1],
                                                      -matrix[...,1,0],matrix[...,0,0]),dim=-1).reshape(*matrix.shape)
 
-def newton_step(f:tuple["B","H","W"], inds, device = "cuda"):
+def newton_step(f, inds, device = "cuda"):
     B,H,W = f.shape
     Hess = finite_diff_hessian(f).reshape(B,H*W,2,2)
     Hess = torch.gather(Hess, dim = 1, index = inds[...,None].expand(B,-1,2,2))
@@ -719,7 +719,7 @@
     mask = ((x_A_to_B > -1) * (x_A_to_B < 1)).prod(dim=-1).float()
     return torch.cat((x_A.expand(*x_A_to_B.shape), x_A_to_B),dim=-1), mask
 
-def dual_log_softmax_matcher(desc_A: tuple['B','N','C'], desc_B: tuple['B','M','C'], inv_temperature = 1, normalize = False):
+def dual_log_softmax_matcher(desc_A, desc_B, inv_temperature = 1, normalize = False):
     B, N, C = desc_A.shape
     if normalize:
         desc_A = desc_A/desc_A.norm(dim=-1,keepdim=True)
@@ -730,7 +730,7 @@
     logP = corr.log_softmax(dim = -2) + corr.log_softmax(dim= -1)
     return logP
 
-def dual_softmax_matcher(desc_A: tuple['B','N','C'], desc_B: tuple['B','M','C'], inv_temperature = 1, normalize = False):
+def dual_softmax_matcher(desc_A, desc_B, inv_temperature = 1, normalize = False):
     if len(desc_A.shape) < 3:
         desc_A, desc_B = desc_A[None], desc_B[None]
     B, N, C = desc_A.shape
@@ -743,7 +743,7 @@
     P = corr.softmax(dim = -2) * corr.softmax(dim= -1)
     return P
 
-def conditional_softmax_matcher(desc_A: tuple['B','N','C'], desc_B: tuple['B','M','C'], inv_temperature = 1, normalize = False):
+def conditional_softmax_matcher(desc_A, desc_B, inv_temperature = 1, normalize = False):
     if len(desc_A.shape) < 3:
         desc_A, desc_B = desc_A[None], desc_B[None]
     B, N, C = desc_A.shape
@@ -756,4 +756,4 @@
     P_B_cond_A = corr.softmax(dim = -1)
     P_A_cond_B = corr.softmax(dim = -2)
     
-    return P_A_cond_B, P_B_cond_A 
\ No newline at end of file
+    return P_A_cond_B, P_B_cond_A 
